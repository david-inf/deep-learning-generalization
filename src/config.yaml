# Template for the configuration file
# first options are common, mostly, between experiments
# so one can try to merge common options with the specific ones

# seed ensures proper reproducibility, especially when resuming
seed: 42
# - data randomization
# - dataset splitting

test_size: 0.2  # train-test split
batch_size: 128
num_workers: 0  # 0 when no augmentations are used

learning_rate: 0.01
momentum: 0.9
weight_decay: 0.
lr_decay: 0.95  # exponential decay at each epoch

log_every: 20  # compute metrics and log to comet_ml after ... batches
batch_window: 50  # use the previous ... batches to compute the metrics

checkpoint_every: null  # save the model every ... epochs

comet_project: "deep-learning-generalization"


model_name: "Net"
num_epochs: 20

label_corruption_prob: 0.  # start with original labels
data_corruption_type: "none"  # start with original data

resume_checkpoint: null

experiment_name: null
experiment_key: null

# flag to indicate if the interpolation was reached
# crucial when resuming, to avoid overwriting the results
interp_reached: false

# when resuming make sure about to update:
# num_epochs
# resume_checkpoint
# experiment_key