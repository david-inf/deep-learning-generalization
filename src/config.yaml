# Template for the configuration file
# first options are common, mostly, between experiments
# so one can try to merge common options with the specific ones

# seed ensures proper reproducibility, especially when resuming
seed: 42
# - data randomization
# - dataset splitting

val_size: 0.1  # train-val split (set 0. to avoid)
batch_size: 128
num_workers: 2
device: cuda

learning_rate: 0.01
momentum: 0.9
weight_decay: 0.
lr_decay: 0.95  # exponential decay at each epoch

comet_project: "deep-learning-generalization"
experiment_name: null
experiment_key: null
checkpoint_every: null  # save the model every ... epochs
log_every: 20  # compute metrics and log to comet_ml after ... batches
resume_checkpoint: null

model_name: "Net"
num_epochs: 10

label_corruption_prob: 0.  # start with original labels
data_corruption_type: "none"  # start with original data

# flag to indicate if the interpolation was reached
# crucial when resuming, to avoid overwriting the results
interp_reached: false

# when resuming make sure about to update:
# num_epochs
# resume_checkpoint
# experiment_key